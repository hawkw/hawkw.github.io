---
layout: post
title:  "Directed Test Generation"
categories: testing,ideas
---

Yesterday, after Dr. Kapfhammer discussed his work on test case generation in class, Max and I had an interesting discussion about automated code generation, which gave me some ideas that I'd like to look into further.

It seems to me that automated generation of test cases, at least for 'higher-level' forms of testing such as integration and acceptance testing, is a fairly difficult problem, since testing deals not only with the _function_ of software but also with its' _purpose_. While it's relatively easy to write a unit of code to accomplish some task without understanding what role that code will play in a larger system, I feel like it would be significantly more difficult to write tests for that code. 

Even if the tests in question are unit tests focused on an individual class or function isolated from the rest of the system, writing effective tests involves contextual information. For example, what kind of inputs is a function likely to recieve? A type signature, such as `public int f(int a)` gives some information that an automated test generator can use, but maybe not enough to write effective tests. We might know that a given function recieves an integer value, but we don't know what kind of integers it is likely to recieve. Are they an even mix of positive and negative, or is one type of value more common? Are they large or small integers?

Max and I also discussed the role that automation plays in the development workflow. Research in automated test generation implies to me a vision of a future in which humans write code, and once that code is written, automated software writes tests to ensure the validity of that code. Since, as we've discussed above, testing often requires contextual information and knowledge of a program's purpose, this seems like a less than ideal solution to me. 

In the programming paradigm of [test-driven development](http://digitalcommons.calpoly.edu/cgi/viewcontent.cgi?article=1034&context=csse_fac&sei-redir=1&referer=https%3A%2F%2Fscholar.google.com%2Fscholar%3Fhl%3Den%26q%3Dtest-driven%2Bdevelopment%26btnG%3D%26as_sdt%3D1%252C39%26as_sdtp%3D#search=%22test-driven%20development%22), or TDD, programmers write tests first, with the expectation that they all will fail, and then write code to make the tests pass. In this paradigm, the tests are viewed _describing_  or _specifying_ the program's behaviour rather than _verifying_ or _testing_ it. It seems to me that the ideal future might not be one where people write code and machines test it, but one where people write tests and machines implement the described functionality.

Now here is where we get to the actual research idea. There exist a number of testing tools, such as [specs2](http://etorreborre.github.io/specs2/) and [ScalaTest](http://www.scalatest.org), in which tests are written as specifications rather than test cases, using embedded strings and natural language-like domain-specific languages. The goal of these tools is essentially to allow a programmer to write a written description of a program's functionality that also happens to be an executable test suite. 

All of this implies to me the idea of a potential extension of automated test generation, which I'm calling _directed test generation_. I'm imagining a system in which human programmers write some high-level descriptions of the behaviour of a program as a whole, and those descriptions are used by an automated system to generate unit tests. Generating unit tests that accurately test the program's behaviour would be easier if some information about it's purpose could be inferred from human-written descriptions. These descriptions could be documentation (perhaps similar to Python's [doctest](https://docs.python.org/2/library/doctest.html)) or tests written in a description language like the ones I discussed above.

There are some obvious questions about the feasibility of this idea. The kind of abstract reasoning necessary to use existing high-level tests to generate more focused low-level tests seems like a difficult artificial intelligence problem to me. However, I don't know very much about how existing test generation systems or the algorithms that power them, so there might be some way they could be modified or enhanced with information collected from human-written tests. It's just an idea, but it seems like it might be looking into further.